{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10328702,"sourceType":"datasetVersion","datasetId":6395340}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flask flask-cors transformers sentence-transformers torch pymupdf pandas optimum auto-gptq pyngrok","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T04:51:34.821469Z","iopub.execute_input":"2024-12-30T04:51:34.821806Z","iopub.status.idle":"2024-12-30T04:51:45.454480Z","shell.execute_reply.started":"2024-12-30T04:51:34.821776Z","shell.execute_reply":"2024-12-30T04:51:45.453662Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\nCollecting flask-cors\n  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nCollecting pymupdf\n  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\nCollecting optimum\n  Downloading optimum-1.23.3-py3-none-any.whl.metadata (20 kB)\nCollecting auto-gptq\n  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting pyngrok\n  Downloading pyngrok-7.2.2-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.4)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\nCollecting coloredlogs (from optimum)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum) (3.2.0)\nRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.34.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq)\n  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting peft>=0.5.0 (from auto-gptq)\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\nCollecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.10.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (4.0.3)\nDownloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading optimum-1.23.3-py3-none-any.whl (424 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.1/424.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyngrok-7.2.2-py3-none-any.whl (22 kB)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rouge, pyngrok, pymupdf, humanfriendly, gekko, huggingface-hub, coloredlogs, flask-cors, sentence-transformers, peft, optimum, auto-gptq\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed auto-gptq-0.7.1 coloredlogs-15.0.1 flask-cors-5.0.0 gekko-1.2.1 huggingface-hub-0.27.0 humanfriendly-10.0 optimum-1.23.3 peft-0.14.0 pymupdf-1.25.1 pyngrok-7.2.2 rouge-1.0.1 sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade transformers huggingface_hub\n!pip install bitsandbytes Flask-CORS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T04:51:45.455826Z","iopub.execute_input":"2024-12-30T04:51:45.456155Z","iopub.status.idle":"2024-12-30T04:52:05.003303Z","shell.execute_reply.started":"2024-12-30T04:51:45.456121Z","shell.execute_reply":"2024-12-30T04:52:05.002275Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nCollecting transformers\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.1\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: Flask-CORS in /usr/local/lib/python3.10/dist-packages (5.0.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from Flask-CORS) (2.2.5)\nRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-CORS) (3.0.4)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-CORS) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-CORS) (2.2.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-CORS) (8.1.7)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->Flask-CORS) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")  # Replace with your actual token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T04:52:05.004794Z","iopub.execute_input":"2024-12-30T04:52:05.005092Z","iopub.status.idle":"2024-12-30T04:52:05.531415Z","shell.execute_reply.started":"2024-12-30T04:52:05.005064Z","shell.execute_reply":"2024-12-30T04:52:05.530764Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"import os\nimport requests\nimport fitz\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom spacy.lang.en import English\nimport re\nimport random\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\nfrom transformers.utils import is_flash_attn_2_available\nfrom timeit import default_timer as timer\nfrom huggingface_hub import login\nfrom huggingface_hub.hf_api import HfFolder\nfrom flask import Flask, request, jsonify  # Import Flask\nfrom pyngrok import ngrok\n\n    \n\nuse_quantization_config = False\n# --- 1. Data Loading and Preprocessing ---\n\ndef download_pdf(url, filename):\n    \"\"\"Downloads a PDF file if it doesn't exist.\"\"\"\n    if not os.path.exists(filename):\n        print(\"File doesn't exist, downloading...\")\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(filename, \"wb\") as file:\n                file.write(response.content)\n            print(f\"The file has been downloaded and saved as {filename}\")\n        else:\n            print(f\"Failed to download the file. Status code: {response.status_code}\")\n    else:\n        print(f\"File {filename} exists.\")\n\ndef text_formatter(text: str) -> str:\n    \"\"\"Performs minor formatting on text.\"\"\"\n    cleaned_text = text.replace(\"\\\\n\", \" \").strip()\n    return cleaned_text\n\ndef open_and_read_pdf(pdf_path: str) -> list[dict]:\n    \"\"\"Opens a PDF, reads text, and collects statistics.\"\"\"\n    doc = fitz.open(pdf_path)\n    pages_and_texts = []\n    for page_number, page in tqdm(enumerate(doc)):\n        text = page.get_text()\n        text = text_formatter(text)\n        pages_and_texts.append({\n            \"page_number\": page_number,\n            \"page_char_count\": len(text),\n            \"page_word_count\": len(text.split(\" \")),\n            \"page_sentence_count_raw\": len(text.split(\". \")),\n            \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars\n            \"text\": text\n        })\n    return pages_and_texts\n\ndef split_into_sentences(pages_and_texts, nlp):\n    \"\"\"Splits text into sentences using spaCy.\"\"\"\n    for item in tqdm(pages_and_texts):\n        item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n        item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n\ndef chunk_sentences(pages_and_texts, chunk_size=10):\n    \"\"\"Recursively splits sentences into chunks.\"\"\"\n    for item in tqdm(pages_and_texts):\n        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"], slice_size=chunk_size)\n        item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n\ndef split_list(input_list: list, slice_size: int) -> list[list[str]]:\n    \"\"\"Splits a list into sublists of a specified size.\"\"\"\n    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n\ndef create_chunk_items(pages_and_texts):\n    \"\"\"Splits each chunk into its own item with stats.\"\"\"\n    pages_and_chunks = []\n    for item in tqdm(pages_and_texts):\n        for sentence_chunk in item[\"sentence_chunks\"]:\n            chunk_dict = {}\n            chunk_dict[\"page_number\"] = item[\"page_number\"]\n            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n            joined_sentence_chunk = re.sub(r'\\\\.([A-Z])', r'. \\\\1', joined_sentence_chunk)\n            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n            pages_and_chunks.append(chunk_dict)\n    return pages_and_chunks\n\n# --- 2. Embedding Creation ---\n\ndef create_embeddings(pages_and_chunks, embedding_model):\n    \"\"\"Creates embeddings for each chunk.\"\"\"\n    for item in tqdm(pages_and_chunks):\n        item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n\n# --- 3. Retrieval ---\n\ndef retrieve_relevant_resources(query, embeddings, model, n_resources_to_return=5, print_time=True):\n    \"\"\"Embeds a query and returns top k scores and indices.\"\"\"\n    query_embedding = model.encode(query, convert_to_tensor=True)\n    start_time = timer()\n    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n    end_time = timer()\n    if print_time:\n        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)\n    return scores, indices\n\ndef print_top_results_and_scores(query, embeddings, pages_and_chunks, n_resources_to_return=5):\n    \"\"\"Retrieves and prints most relevant resources.\"\"\"\n    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, n_resources_to_return=n_resources_to_return)\n    print(f\"Query: {query}\\n\")\n    print(\"Results:\")\n    for score, index in zip(scores, indices):\n        print(f\"Score: {score:.4f}\")\n        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n        print(f\"Page number: {pages_and_chunks[index]['page_number']}\\n\")\n\n# --- 4. Prompt Engineering ---\n\ndef prompt_formatter(query: str, \n                     context_items: list[dict]) -> str:\n    \"\"\"\n    Augments query with text-based context from context_items.\n    \"\"\"\n    # Join context items into one dotted paragraph\n    context = \"- \" + \"\\\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n\n    # Create a base prompt with examples related to the Domestic Violence Act\n    base_prompt = \"\"\"Based on the following context items from \"THE PROTECTION OF WOMEN FROM DOMESTIC VIOLENCE ACT, 2005\", please answer the query.\nGive yourself room to think by extracting relevant passages from the context before answering the query.\nDon't return the thinking, only return the answer.\nMake sure your answers are accurate and directly address the legal provisions of the Act.\nUse the following examples as reference for the ideal answer style.\n\nExample 1:\nQuery: What is a \"Protection Order\" under the Act?\nAnswer: A \"Protection Order\" under the Act is an order granted in terms of section 18. It prohibits the respondent from committing any act of domestic violence, aiding or abetting such acts, entering the victim's place of employment or other frequented places, attempting to communicate with the victim, alienating assets, or causing violence to dependants or relatives.\n\\\\nExample 2:\nQuery: Who can file an application for a protection order?\nAnswer: An application for a protection order can be filed by the aggrieved person (the victim of domestic violence) or a Protection Officer or any other person on behalf of the aggrieved person, as per Section 12 of the Act.\n\\\\nExample 3:\nQuery: What kind of relief can be granted under a \"residence order\"?\nAnswer: Under a \"residence order\" (Section 19), the Magistrate can restrain the respondent from dispossessing the aggrieved person from the shared household, direct the respondent to remove themselves, restrain the respondent or their relatives from entering any portion of the shared household where the aggrieved person resides, restrain the respondent from alienating or disposing of the shared household, or direct the respondent to secure alternate accommodation or pay rent for the same.\n\\\\nNow use the following context items to answer the user query:\n{context}\n\n\\\\nRelevant passages: <extract relevant passages from the context here>\nUser query: {query}\nAnswer:\"\"\"\n\n    # Update base prompt with context items and query   \n    base_prompt = base_prompt.format(context=context, query=query)\n\n    # Create prompt template for instruction-tuned model\n    dialogue_template = [\n        {\"role\": \"user\",\n        \"content\": base_prompt}\n    ]\n\n    # Apply the chat template\n    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n                                          tokenize=False,\n                                          add_generation_prompt=True)\n    return prompt\n\n# --- 5. LLM Generation ---\n\ndef ask(query, llm_tokenizer, llm_model, embeddings, pages_and_chunks, temperature=0.7, max_new_tokens=512, format_answer_text=True, return_answer_only=True):\n    \"\"\"Generates an answer to the query based on relevant resources.\"\"\"\n    \n    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, model=embedding_model)\n    context_items = [pages_and_chunks[i] for i in indices]\n    for i, item in enumerate(context_items):\n        item[\"score\"] = scores[i].cpu()\n\n    prompt = prompt_formatter(query=query, context_items=context_items)\n    input_ids_dict = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    # Access the actual input_ids tensor from the dictionary\n    input_ids = input_ids_dict[\"input_ids\"]\n    outputs = llm_model.generate(input_ids, temperature=temperature, do_sample=True, max_new_tokens=max_new_tokens)\n    output_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    if format_answer_text:\n        output_text = output_text.replace(prompt, \"\").replace(\"Sure, here is the answer to the user query:\\\\n\\\\n\", \"\")\n\n    if return_answer_only:\n        return output_text\n\n    return output_text, context_items\n\n# --- Main Execution ---\napp = Flask(__name__)\n\n@app.route('/ask_llm', methods=['POST'])\ndef ask_llm_endpoint():\n    \"\"\"API endpoint to receive queries and return answers.\"\"\"\n    data = request.get_json()\n    if not data or 'query' not in data:\n        return jsonify({'error': 'No query provided'}), 400\n\n    query = data['query']\n    print(f\"Received query: {query}\")\n\n    try:\n        answer = ask(query=query,\n                     llm_tokenizer=tokenizer,\n                     llm_model=llm_model,\n                     embeddings=embeddings,\n                     pages_and_chunks=pages_and_chunks_over_min_token_len,\n                     format_answer_text=True,\n                     return_answer_only=True,\n                     max_new_tokens=64)  # Or your desired max_new_tokens\n        print(f\"Generated answer: {answer}\")\n        return jsonify({'answer': answer})\n    except Exception as e:\n        print(f\"Error during processing: {e}\")\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == \"__main__\":\n    # 1. Load and preprocess the PDF\n    pdf_path = \"/kaggle/input/xcode-api/TheProtectionofWomenfromDomesticViolenceAct2005_0 (1).pdf\"\n    pdf_url = \"http://ncw.nic.in/sites/default/files/TheProtectionofWomenfromDomesticViolenceAct2005_0.pdf\" # Replace with the actual URL\n    download_pdf(pdf_url, pdf_path)\n\n    pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n    nlp = English()\n    nlp.add_pipe(\"sentencizer\")\n    split_into_sentences(pages_and_texts, nlp)\n    chunk_size = 10\n    chunk_sentences(pages_and_texts, chunk_size)\n\n    min_token_length = 30\n    df = pd.DataFrame(pages_and_texts)\n    pages_and_chunks = create_chunk_items(pages_and_texts)\n    pages_and_chunks_over_min_token_len = [chunk for chunk in pages_and_chunks if chunk[\"chunk_token_count\"] > min_token_length]\n    \n    # 2. Create embeddings\n    embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n    create_embeddings(pages_and_chunks_over_min_token_len, embedding_model)\n\n    # Convert list of dictionaries to DataFrame and then to list of dictionaries with tensors\n    df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n    pages_and_chunks = df.to_dict(orient=\"records\")\n\n    # Convert embeddings to tensor\n    embeddings = torch.tensor([item['embedding'] for item in pages_and_chunks_over_min_token_len]).to(\"cuda\")\n\n    # 3. & 4. Load LLM and Tokenizer (Gemma 7B-IT)\n    \n    # 1. Create quantization config for smaller model loading (optional)\n    # Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n    # For models that require 4-bit quantization (use this if you have low GPU memory available)\n    \n    quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n                                            bnb_4bit_compute_dtype=torch.float16)\n\n    # Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n    # Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n    # Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention\n    if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n        attn_implementation = \"flash_attention_2\"\n    else:\n        attn_implementation = \"sdpa\"\n    print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n\n    # 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n    #model_id = \"google/gemma-7b-it\"\n    model_id = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\" # (we already set this above)\n    print(f\"[INFO] Using model_id: {model_id}\")\n\n    # 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n\n    # Load the model configuration\n    config = AutoConfig.from_pretrained(model_id)\n\n    # Load the model\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        config=config,\n        torch_dtype=torch.bfloat16,  # Use bfloat16 data type\n        device_map=\"auto\",              # Automatically choose the device (CPU or GPU)\n        #low_cpu_mem_usage=False,        # Use full memory\n    )\n\n    # 5. Test the RAG pipeline with queries\n    # Nutrition-style questions generated with GPT4\n    format_answer_text = True\n    return_answer_only = True\n\n    # Replace the nutrition-style questions with questions relevant to your PDF\n    gpt4_questions = [\n        \"I am being abused how can you Law help me \",\n        #\"What is the definition of domestic violence under the Act?\",\n        \n    ]\n\n    manual_questions = [\n        \"I am being abused how can you Law help me \",\n        #\"Who is considered an aggrieved person under this law?\",\n        #\"What are the different types of protection orders that can be issued?\",\n        #\"What are the duties of a Protection Officer?\",\n        \n        #\"How can a Magistrate provide residence orders under the Act?\"\n    ]\n    query_list = gpt4_questions + manual_questions\n    query = random.choice(query_list)\n    print(f\"Query: {query}\")\n    max_new_tokens = 64 \n    # Use the ask function to get the answer\n    \n    answer = ask(query=query,\n                 llm_tokenizer=tokenizer,\n                 llm_model=llm_model,\n                 embeddings=embeddings,\n                 pages_and_chunks=pages_and_chunks_over_min_token_len,\n                 format_answer_text=format_answer_text,\n                 return_answer_only=return_answer_only,\n                 max_new_tokens=max_new_tokens)\n    print(answer)\n    \n    query = random.choice(query_list)\n    print(f\"Query: {query}\")\n\n    # Get just the scores and indices of top related results\n    scores, indices = retrieve_relevant_resources(query=query,\n                                                embeddings=embeddings,\n                                                model=embedding_model)\n    # Create a list of context items\n    context_items = [pages_and_chunks[i] for i in indices]\n\n    # Add score to context item\n    for i, item in enumerate(context_items):\n        item[\"score\"] = scores[i].cpu() # return score back to CPU\n\n    # Format the prompt with context items\n    prompt = prompt_formatter(query=query,\n                            context_items=context_items)\n    print(prompt)\n    # Generate an output of tokens\n    input_ids_dict = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    # Pass the input_ids tensor to the generate method\n    outputs = llm_model.generate(input_ids_dict[\"input_ids\"],\n                                 temperature=0.7,\n                                 do_sample=True,\n                                 max_new_tokens=256)\n\n    # Decode the output tokens into text\n    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    if format_answer_text:\n        # Replace special tokens and unnecessary help message\n        output_text = output_text.replace(prompt, \"\").replace(\"Sure, here is the answer to the user query:\\\\n\\\\n\", \"\")\n\n    # Only return the answer without the context items\n    if return_answer_only:\n        print(output_text)\n\n    try:\n        from pyngrok import ngrok\n        ngrok.set_auth_token(\"2quZR2zpDjakOfXzCrc2ChMVjml_7WjmTjzMBGuoFPJt9DQqF\")\n        tunnel = ngrok.connect(5000)\n        print(\"Ngrok tunnel created:\")\n        print(f\"Public URL: {tunnel.public_url}\")\n    except ImportError:\n        print(\"pyngrok not installed. Please install it with: pip install pyngrok\")\n    except Exception as e:\n        print(f\"Error setting up ngrok: {e}\")\n    # --- End of ngrok setup ---\n\n    app.run(host='0.0.0.0', port=5000, debug=False)","metadata":{}},{"cell_type":"code","source":"import os\nimport requests  # Keep this import in case you need it for other things later\nimport fitz\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom spacy.lang.en import English\nimport re\nimport random\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\nfrom transformers.utils import is_flash_attn_2_available\nfrom timeit import default_timer as timer\nfrom huggingface_hub import login\nfrom huggingface_hub.hf_api import HfFolder\nfrom flask import Flask, request, jsonify\nfrom pyngrok import ngrok\nfrom flask_cors import CORS \n\nuse_quantization_config = False\n\n# --- 1. Data Loading and Preprocessing ---\n\ndef text_formatter(text: str) -> str:\n    \"\"\"Performs minor formatting on text.\"\"\"\n    cleaned_text = text.replace(\"\\\\n\", \" \").strip()\n    return cleaned_text\n\ndef open_and_read_pdf(pdf_path: str) -> list[dict]:\n    \"\"\"Opens a PDF, reads text, and collects statistics.\"\"\"\n    doc = fitz.open(pdf_path)\n    pages_and_texts = []\n    for page_number, page in tqdm(enumerate(doc)):\n        text = page.get_text()\n        text = text_formatter(text)\n        pages_and_texts.append({\n            \"page_number\": page_number,\n            \"page_char_count\": len(text),\n            \"page_word_count\": len(text.split(\" \")),\n            \"page_sentence_count_raw\": len(text.split(\". \")),\n            \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars\n            \"text\": text\n        })\n    return pages_and_texts\n\ndef split_into_sentences(pages_and_texts, nlp):\n    \"\"\"Splits text into sentences using spaCy.\"\"\"\n    for item in tqdm(pages_and_texts):\n        item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n        item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n\ndef chunk_sentences(pages_and_texts, chunk_size=10):\n    \"\"\"Recursively splits sentences into chunks.\"\"\"\n    for item in tqdm(pages_and_texts):\n        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"], slice_size=chunk_size)\n        item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n\ndef split_list(input_list: list, slice_size: int) -> list[list[str]]:\n    \"\"\"Splits a list into sublists of a specified size.\"\"\"\n    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n\ndef create_chunk_items(pages_and_texts):\n    \"\"\"Splits each chunk into its own item with stats.\"\"\"\n    pages_and_chunks = []\n    for item in tqdm(pages_and_texts):\n        for sentence_chunk in item[\"sentence_chunks\"]:\n            chunk_dict = {}\n            chunk_dict[\"page_number\"] = item[\"page_number\"]\n            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n            joined_sentence_chunk = re.sub(r'\\\\.([A-Z])', r'. \\\\1', joined_sentence_chunk)\n            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n            pages_and_chunks.append(chunk_dict)\n    return pages_and_chunks\n\n# --- 2. Embedding Creation ---\n\ndef create_embeddings(pages_and_chunks, embedding_model):\n    \"\"\"Creates embeddings for each chunk.\"\"\"\n    for item in tqdm(pages_and_chunks):\n        item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n\n# --- 3. Retrieval ---\n\ndef retrieve_relevant_resources(query, embeddings, model, n_resources_to_return=5, print_time=True):\n    \"\"\"Embeds a query and returns top k scores and indices.\"\"\"\n    query_embedding = model.encode(query, convert_to_tensor=True)\n    start_time = timer()\n    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n    end_time = timer()\n    if print_time:\n        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)\n    return scores, indices\n\ndef print_top_results_and_scores(query, embeddings, pages_and_chunks, n_resources_to_return=5):\n    \"\"\"Retrieves and prints most relevant resources.\"\"\"\n    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, n_resources_to_return=n_resources_to_return)\n    print(f\"Query: {query}\\n\")\n    print(\"Results:\")\n    for score, index in zip(scores, indices):\n        print(f\"Score: {score:.4f}\")\n        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n        print(f\"Page number: {pages_and_chunks[index]['page_number']}\\n\")\n\n# --- 4. Prompt Engineering ---\n\ndef prompt_formatter(query: str, \n                     context_items: list[dict]) -> str:\n    \"\"\"\n    Augments query with text-based context from context_items.\n    Adds ANSWER_START delimiter for easier extraction.\n    \"\"\"\n    # Join context items into one dotted paragraph\n    context = \"- \" + \"\\\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n\n    # Create a base prompt with examples related to the Domestic Violence Act\n    base_prompt = \"\"\"Based on the following context items from \"THE PROTECTION OF WOMEN FROM DOMESTIC VIOLENCE ACT, 2005\", please answer the query.\nGive yourself room to think by extracting relevant passages from the context before answering the query.\nDon't return the thinking, only return the answer.\nMake sure your answers are accurate and directly address the legal provisions of the Act.\nUse the following examples as reference for the ideal answer style.\n\nExample 1:\nQuery: What is a \"Protection Order\" under the Act?\nAnswer: A \"Protection Order\" under the Act is an order granted in terms of section 18. It prohibits the respondent from committing any act of domestic violence, aiding or abetting such acts, entering the victim's place of employment or other frequented places, attempting to communicate with the victim, alienating assets, or causing violence to dependants or relatives.\n\\\\nExample 2:\nQuery: Who can file an application for a protection order?\nAnswer: An application for a protection order can be filed by the aggrieved person (the victim of domestic violence) or a Protection Officer or any other person on behalf of the aggrieved person, as per Section 12 of the Act.\n\\\\nExample 3:\nQuery: What kind of relief can be granted under a \"residence order\"?\nAnswer: Under a \"residence order\" (Section 19), the Magistrate can restrain the respondent from dispossessing the aggrieved person from the shared household, direct the respondent to remove themselves, restrain the respondent or their relatives from entering any portion of the shared household where the aggrieved person resides, restrain the respondent from alienating or disposing of the shared household, or direct the respondent to secure alternate accommodation or pay rent for the same.\n\\\\nNow use the following context items to answer the user query:\n{context}\n\nQuery: {query}\nANSWER_START\"\"\"\n\n    # Update base prompt with context items and query   \n    base_prompt = base_prompt.format(context=context, query=query)\n\n    # Create prompt template for instruction-tuned model\n    dialogue_template = [\n        {\"role\": \"user\",\n        \"content\": base_prompt}\n    ]\n\n    # Apply the chat template\n    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n                                          tokenize=False,\n                                          add_generation_prompt=True)\n    return prompt\n\ndef ask(query, llm_tokenizer, llm_model, embeddings, pages_and_chunks, temperature=0.7, max_new_tokens=512, format_answer_text=True, return_answer_only=True):\n    \"\"\"Generates an answer to the query based on relevant resources.\"\"\"\n    try:\n        scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, model=embedding_model)\n        context_items = [pages_and_chunks[i] for i in indices]\n        for i, item in enumerate(context_items):\n            item[\"score\"] = scores[i].cpu()\n\n        prompt = prompt_formatter(query=query, context_items=context_items)\n        input_ids_dict = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        input_ids = input_ids_dict[\"input_ids\"]\n        outputs = llm_model.generate(input_ids, temperature=temperature, do_sample=True, max_new_tokens=max_new_tokens)\n        output_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        if format_answer_text:\n            # Extract everything after ANSWER_START\n            if \"ANSWER_START\" in output_text:\n                output_text = output_text.split(\"ANSWER_START\")[1].strip()\n            # Remove any additional prefixes/templates\n            output_text = output_text.replace(\"assistant\", \"\").replace(\"user\", \"\").strip()\n            # Clean up any remaining prompt artifacts\n            output_text = re.sub(r'Example \\d+:|Query:|Answer:', '', output_text).strip()\n\n        if return_answer_only:\n            return output_text\n\n        return output_text, context_items\n        \n    except Exception as e:\n        print(f\"Error in ask function: {str(e)}\")\n        return \"I apologize, but I encountered an error while processing your query. Please try again.\"\n\n# --- Main Execution ---\napp = Flask(__name__)\nCORS(app, resources={r\"/ask_llm\": {\"origins\": \"http://localhost:3000\"}})\n\n@app.route('/ask_llm', methods=['POST'])\ndef ask_llm_endpoint():\n    \"\"\"API endpoint to receive queries and return answers.\"\"\"\n    data = request.get_json()\n    if not data or 'query' not in data:\n        return jsonify({'error': 'No query provided'}), 400\n\n    query = data['query']\n    print(f\"Received query: {query}\")\n\n    try:\n        answer = ask(query=query,\n                     llm_tokenizer=tokenizer,\n                     llm_model=llm_model,\n                     embeddings=embeddings,\n                     pages_and_chunks=pages_and_chunks_over_min_token_len,\n                     format_answer_text=True,\n                     return_answer_only=True,\n                     max_new_tokens=256)  # Or your desired max_new_tokens\n        print(f\"Generated answer: {answer}\")\n        return jsonify({'answer': answer})\n    except Exception as e:\n        print(f\"Error during processing: {e}\")\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == \"__main__\":\n    # Define the paths to your 6 PDF files\n    pdf_paths = [\n        \"/kaggle/input/xcode-api/2.pdf\",  # Replace with your actual paths\n        \"/kaggle/input/xcode-api/THEDOWRYPROHIBITIONACT1961_0.pdf\",\n        \"/kaggle/input/xcode-api/SexualHarassmentofWomenatWorkPlaceAct2013_0.pdf\",\n        \"/kaggle/input/xcode-api/THEIMMORALTRAFFIC(PREVENTION)ACT1956_2.pdf\",\n        \"/kaggle/input/xcode-api/TheCommissionofSatiPreventionAct1987-of1988_0.pdf\",\n        \"/kaggle/input/xcode-api/TheIndecentRepresentationofWomenProhibitionAct1986_2.pdf\",\n        \"/kaggle/input/xcode-api/TheProtectionofWomenfromDomesticViolenceAct2005_0 (1).pdf\",\n        \"/kaggle/input/xcode-api/The_Criminal_Law_Amendment_Act_2013_0.pdf\"\n    ]\n\n    all_pages_and_chunks = []\n\n    for pdf_path in pdf_paths:\n        print(f\"Processing: {pdf_path}\")\n        pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n        nlp = English()\n        nlp.add_pipe(\"sentencizer\")\n        split_into_sentences(pages_and_texts, nlp)\n        chunk_size = 10\n        chunk_sentences(pages_and_texts, chunk_size)\n\n        min_token_length = 30\n        df = pd.DataFrame(pages_and_texts)\n        pages_and_chunks = create_chunk_items(pages_and_texts)\n        pages_and_chunks_over_min_token_len = [\n            chunk for chunk in pages_and_chunks if chunk[\"chunk_token_count\"] > min_token_length\n        ]\n        all_pages_and_chunks.extend(pages_and_chunks_over_min_token_len)\n\n    # 2. Create embeddings for all chunks\n    embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n    create_embeddings(all_pages_and_chunks, embedding_model)\n\n    # Convert list of dictionaries to DataFrame and then to list of dictionaries with tensors\n    df = pd.DataFrame(all_pages_and_chunks)\n    pages_and_chunks_over_min_token_len = df.to_dict(orient=\"records\")\n\n    # Convert embeddings to tensor\n    embeddings = torch.tensor([item['embedding'] for item in pages_and_chunks_over_min_token_len]).to(\"cuda\")\n\n    # 3. & 4. Load LLM and Tokenizer (Gemma 7B-IT)\n    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n    if is_flash_attn_2_available() and (torch.cuda.get_device_capability(0)[0] >= 8):\n        attn_implementation = \"flash_attention_2\"\n    else:\n        attn_implementation = \"sdpa\"\n    print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n    model_id = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n    print(f\"[INFO] Using model_id: {model_id}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    config = AutoConfig.from_pretrained(model_id)\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        model_id, config=config, torch_dtype=torch.bfloat16, device_map=\"auto\"\n    )\n\n    # --- ngrok setup ---\n    try:\n        ngrok.set_auth_token(\"2quZR2zpDjakOfXzCrc2ChMVjml_7WjmTjzMBGuoFPJt9DQqF\")\n        tunnel = ngrok.connect(5000)\n        print(\"Ngrok tunnel created:\")\n        print(f\"Public URL: {tunnel.public_url}\")\n    except ImportError:\n        print(\"pyngrok not installed. Please install it with: pip install pyngrok\")\n    except Exception as e:\n        print(f\"Error setting up ngrok: {e}\")\n    # --- end ngrok setup ---\n\n    app.run(host='0.0.0.0', port=5000, debug=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T04:52:05.532291Z","iopub.execute_input":"2024-12-30T04:52:05.532499Z","iopub.status.idle":"2024-12-30T05:58:36.184395Z","shell.execute_reply.started":"2024-12-30T04:52:05.532481Z","shell.execute_reply":"2024-12-30T05:58:36.183397Z"}},"outputs":[{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/2.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f44f5b52517544a69928a05f607706b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bd7ca24b4c4266bdca77661ef3dd6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a332982a54440e8bb0ea7a321406876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aba921ce2c44ae0a3f40564cc902541"}},"metadata":{}},{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/THEDOWRYPROHIBITIONACT1961_0.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bea94a9d0404f8591b71c84d509af52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb9c7a6d7a8d4f42845d070290df4847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae47201eb3054630a34d9c64a005d53f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba9f98e06334380b0b5cbe2881a03d8"}},"metadata":{}},{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/SexualHarassmentofWomenatWorkPlaceAct2013_0.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebaa61b6107e4da0b0a81b99252c5bb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"154bf27f27e6412bb1df3051d4251278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5335730f291d4d01b73943dd9093931f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7611827dc31545aea50a0ce232454e9c"}},"metadata":{}},{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/THEIMMORALTRAFFIC(PREVENTION)ACT1956_2.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"956166d3fd8144fba95e3237b1bef407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8a705057704c788cafb02474a7b5b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da7b61e06c524e2fa394d979a21382a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"123b541f723c457fb294d4d792ca5d01"}},"metadata":{}},{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/TheCommissionofSatiPreventionAct1987-of1988_0.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec21bdcb6de46b5b8fcf2e43035f98a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"716efdab25b3430b800f96f224ca69aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee430370df9413a9b4a72ebdafb7b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81644b7fa95b462c9edece44b9067bef"}},"metadata":{}},{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/TheIndecentRepresentationofWomenProhibitionAct1986_2.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43307cfe38c34cf0a2de244030693e5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29490128745f4052a0372a4a3e3f536a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf78806321e4134b8800c8ba06a1f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3fba97eba1f458c96f306324b0a52cb"}},"metadata":{}},{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/TheProtectionofWomenfromDomesticViolenceAct2005_0 (1).pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb2460872f84e6db0f86c638a17d2eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a40c2e9070e4e59ba118f88de11f75f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a307df53876435c94327ab62cf02115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2121f5af2f054a54bcbc871608796195"}},"metadata":{}},{"name":"stdout","text":"Processing: /kaggle/input/xcode-api/The_Criminal_Law_Amendment_Act_2013_0.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"141dd514bdd04cbe8f7100670a3958cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"255eed7f1d5546d08d394d70332b7fc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4942a70386f4457dbca534a9303ba1ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f9860fae104a25a16491e78e6705d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6387e320a3524345bcf787319bc9fab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f49261bd9bf4bc09c37a1d399fc5508"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f230c33f2b046d68b7ff98577c92889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"911f4eea6dd04fedac7844cea41f40e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2da6b38e29143ac8073895fe09e4e67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117248d1ab2d419dae1cd8ff76d4498f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da039b75d7ec466284b145b56e7f0818"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"185bef4ae0124bc593bd913753be4b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e094963f23474cb7802cb927711ebb45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38796b5c7fc4818a4a196b97bcb8007"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4ba707232964f16afd2017cc21f2b3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/168 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"142b3657c3364b7aae18ad253efc4e23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b26c9694a86e4a3ab53187e894bf80ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a854201be6a146c4ba621c4be00566de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad95449cbdc4f0ea371479bd6984284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa45b28ac937468facd8339aa5892456"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e739832219df4b36b56cf3de32f8091c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d2497fe2a943e7a06a869c1830865c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6bfc4c128ed4a4ab1496e8e51f00e1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4ac8435f0f46e587e7269cfcf4d6aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edc83a5a732945c9b0ebd2288079ef5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73cce64fe96433ca129cba61313dc1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31574734d8045dc83bf4a80e4b5dd38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb7f4fafcc444519f9b1793b4e21d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"726337d3febc40cf8c18d8f9538818fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be4886e9a3f4e91b6704f1162577aff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860c293ea5ba42fbab1ade0885ed8e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e4edea4d8f412cad64028dcff6f44f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ca882284c37485b8b91c1642f1bbada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2e6c8b159b4e5b938c122cb615ff93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1790bd9b6804be69272b37f744f993e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea94ab99dc474eac9577a55784892d77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3554ea3e2858417a86f1278028165cca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d9cb042e06b47a59cef4de9c74120c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176ba6f29ed44c429f07160f5435af17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77274cde112e48e5856cbccd9360034a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da480e337c5548c08328f54f0c6f550b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e3eb4118524f94a88f07af54613e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf2a6ad21e1443db994e242f13a02076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dcdc9a498b04af3a2bd11a468a8d042"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e06f50763c94b69b34d86ff7dafe00b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5221a3941e394a4a89f038892d64618e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"930e7c640b434631ac289b6dec25c801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639fcbbb3e5648788a3db72107700a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fbcb667e6e64977b0475e8f5cdc0d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fde62a523dec45af898883b974b277d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0bbc44e2ce34813bb9f5f0c4a5e9bd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2adda07db14e7a8bca56d4f4e67629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ae80bb46564bf5aae2720b28a53a64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085b2d3a9b55420a83cff3868c0111ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c7a00981bd405b969815e9aca3ee35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f32111a7e4d4ae2b797d5edb4f1e840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d2ccfe3152047208f93bfc8abb41f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d6d706dd284dd881c02ffee44685c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"253b4c1728c5445c85ce2daefcfb5e70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b038aae97f4b76b94b2fa05dae18a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41d8153a3824f328d1c4ced9f0ed448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84955db25b224d4a8b54a050789ae7d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad20d4d573540c4b538fb74e58657b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eb337e4aa414d68957c3bf8448f068a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b9de772a1cd492c88362acbb5776ea0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aad671bb42d5483cbe1834e221c6115c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"566a2dee1ee24e3fb8db89fdd1984a41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ee5850593841c0b5c2bd451d11a2b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ffc5d4a5ef64eff840425dde9619033"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693cafcdc01d4bb1b124b64dc56df237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2d3d33de1e4abcb9c4e8adb3726a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475c41141bc54af480c6f5971c334b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"844300076043408c8e743c1ad1dac3f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbdf46a811aa415b8b20d751919391d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3689529dcfb54bd28d9479bcd50dd7d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1e2d64c1d384bd4be541a558a6b5029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52fa089f983c42438968ec93bcdf85cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4864edcebca541638b0b51d87e564745"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604cc7e6ceea420f88fbd0ca451c7120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59518ee85f4a401c9bef43c0885fbccb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17eabd287785457bb8be10f62f2df14d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ec150c226434c359661356bd66f27e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86e2293529bd42c386be548c35a91f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af4a25b89cc04227b03d8b4db1ce282a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"609ee163b8574c07b10529302183e492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb79e05dac44a87bf72be3f31d2cdb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d1c449965a457a89d71f0d3614bd14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a393bbb49945ab882cbee7a4be3f76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a47a287a1c413792b1cdac29ceeb03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f097bcb8e24ce8b10947631474afd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82341e61b9f42dd929e6d627b4714c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"926df68edcb74af9a09efd86aba7dc12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f431013d40b3499b8f73757575ce936f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2acab05c80d44db9ffd929f18cebf22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90fb2bd942b1454a9a0015f76db8f02b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc75e6666664dc9823c58209a672f7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2aca7b1541d4401a60ac1325999fda8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"444b9245ca264819b012ff7bcbb87c91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c378453dd1b4cffabb406f178249fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d33533347134d24814fa2756bfcd7a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c179ba583d34a18ac70c6f2f86a941e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91e2a8b98c1f4eacbfd64cf73ad0acb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab3b10f9da5d410e8398a3ae7161bc7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2b3f359673c4d8982417d4c5917c022"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2111ef40ca94fdaa28657869d3fc8f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ba04fbbda044b18cbe10f5101bbfdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da713522c4a54bd29001725231e006cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6d5bedd83c4befaf87753b39a84019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c81681bb4446e7ada548e423aaf902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e52d0d95a548a1bbe17824cabd80b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de4550f6900d4907ad5331d4f8940c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"292c6137418d4f68a02ccc878f248a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a245f4171057439c8159c451ab8b41cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"555892a15c554a159d7be95c1e00cc88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"550377b533a04d5c9affe84400d21534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffbeed72326e454ebdff64b3854372a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52c31280ac14406b2b7d8f060fb55e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9619b85db8b428c89eaa5bc10070f65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9132253c09064d2c9667fc23d329ffea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1794d8ff98d04b268d41bb6002b29c30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4aa5d83e74a4d2f89457e4cc260906d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de6122bc7e7d41f098ee45ca306514c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceb5735c55694b4caedfbe527915e53a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca0d4e3ff87743fea52120e3583ee810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd4211a92f440a2913e53a3b7879982"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4d2e1cfee74674a62a2961849d4119"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40348c2445fd4c6697785bbed506df91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7724343d77f49b7b824c5dcc60ed9b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e8f159d8a14ad3b64de94f17ccb896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab1b6e7315c1482491fb32bc2bacdca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e583d1b2ed14b16ad1f34c7bbc6c1cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9367795c659d4db6a993e7605748e935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3310a303f53409a9c4c1b076d9ab300"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9a3c6fe58b49f99a324febb55c26db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4da2b2acb6b94fc292afd30c4589635e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c858dd7dfe9a4b90870327f4951d33b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a0e323d30e44c0a5a858dcb9f18b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a132c4472aa14aecb49f8508db2dd280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0dc78aff8284c17b67ded0124e68de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ecb52d443e471f93c973c2e10048f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74f1ab0ba91e41c1b98527f8e230608d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ca21c450ca45509164e754abbfe9fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af3b86c203bc4e6eb033f337d4c5c790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494b6b87bec6456389f526b629594520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da10d385d8a74512a56e6af7af29ad70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93195c3b25af4cd4a9552e97b2446a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64199a279d946f289617dbbddd9127d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6006797dad924f98a73859100606e632"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86a4d27c8645436da8b69b9f05800fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d595b7b2db44bd7bc0b07e40ef59721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82477de13b949a787f4205e78c91406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bccfbd3e7991473eb5ffd5bca1ecea78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15ef55f5585744128e2300a460271668"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9a396c46ad4ef4aa8f3ca42314c468"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b53207cf0c4b0897341be35734b993"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82544f2cf87a4571810968adbf6b22ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a16cfc91474043885a6c0ae8f01811"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9add958ad0427bae5ea969e3387376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"413f4876f641446aba4c8d5d6a68a803"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd9a921edf64b37a7170272b64895d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d191fc482a734d32bd081068f23332b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b483b5b2424a4450adbb5cb7cfcc6067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfc6bca3b76d48cd9dd5068f47858f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30dc64c47b1b4e4f9fb47d5ff0bcce6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb5f941d282841dc913528d042b0228a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ede1539bdb34244bbc659809bd03a54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"831828e2caa84bdb98428f34f76aff60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121b66aba462496f80d1d2c68e5db5b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e39fc6c29c4aaeb8bf3725a52199e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5e87f76c49421dbd1b9b82f381cf64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe46610362944cda7afc77e2e0543b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a8e271bcae347d18e628e6afdbb1afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fda6408624364b0c9f01ed507697ecdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf353503303a470eaf3dc4be563d7dab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2663fd6e66e342c9b01ce269bf3f13a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"736ffeb9ae934016a36be7c93d8c8390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcfa70d7ee7f4d9cbe2ee5d9ca219f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5837c53d7a44fbb84abf23c7afe3388"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187148afb98348caaee18e7ba963e0d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feea2a3d145e4a4e8df04a7bf53655c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb95cdd3ffde426997dac23080f9f616"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46a8d6367ab74d9dbff3bb203ae8f114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ce6ed561c14c6398221dbbf3ac419a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9afd8f6ab1c4fb9aa8d7598ece50d5f"}},"metadata":{}},{"name":"stdout","text":"[INFO] Using attention implementation: sdpa\n[INFO] Using model_id: TheBloke/Mistral-7B-OpenOrca-GPTQ\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-abadb05b13c4>:259: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  embeddings = torch.tensor([item['embedding'] for item in pages_and_chunks_over_min_token_len]).to(\"cuda\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933ad290fef2424e8d05d3b4bb041511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd780cbfc1d24b3994e148821c72c387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f40701a68eb14d2b9fa88ce0be9c1ddf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3156c77db64c4e66a87855924e862d23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e852d50333b740a8af5798d8c64dd029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302a784a7f8a43929cfec6f9f6c2642b"}},"metadata":{}},{"name":"stderr","text":"Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c50e90ccce4917bfeb4525cd0933df"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5055: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nSome weights of the model checkpoint at TheBloke/Mistral-7B-OpenOrca-GPTQ were not used when initializing MistralForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c22bf82adc5643f886a9aa34801b508b"}},"metadata":{}},{"name":"stdout","text":"Ngrok tunnel created:                                                                               \nPublic URL: https://9fff-34-168-131-109.ngrok-free.app\n * Serving Flask app '__main__'\n * Debug mode: off\nReceived query: Women Rights\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2b28da79a24447a8f58ad7e2ca930d4"}},"metadata":{}},{"name":"stdout","text":"[INFO] Time taken to get scores on 168 embeddings: 0.00213 seconds.\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Generated answer: The Protection of Women from Domestic Violence Act, 2005 is a comprehensive legislation in India that aims to protect women from domestic violence and ensure their rights. Some of the key provisions in the Act include:\n\n1. Definition of Domestic Violence: The Act defines domestic violence as any act of physical, verbal, emotional, economic, and sexual abuse committed by a respondent against the aggrieved person, leading to significant harm to her health, safety, or well-being.\n\n2. Protection Orders: A magistrate may grant a Protection Order to protect the aggrieved person from the respondent's acts of domestic violence, restraining the respondent from committing any act of domestic violence or alienating assets.\n\n3. Residence Orders: The Act allows the magistrate to issue a residence order, which may include restraining the respondent from dispossession or alienation of the shared household, directing the respondent to remove themselves, or ordering the respondent to secure alternate accommodation or pay rent for the same.\n\n4. Monetary Relief: The magistrate can order the respondent to pay a lump sum amount to the aggrieved\nReceived query: Women Rights\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc2e75be5154b2487f7ff8c0774c9cd"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INFO] Time taken to get scores on 168 embeddings: 0.00057 seconds.\nGenerated answer: The Protection of Women from Domestic Violence Act, 2005 aims to provide women with certain rights and protections against domestic violence. These rights and protections are outlined in various sections of the Act, including:\n\n1. Right to residence: Section 11 of the Act grants the aggrieved person (the victim) the right to reside in the shared household, whether or not the relationship between the parties has ended.\n\n2. Right to protection: Section 18 of the Act mandates a Protection Order, which prohibits the respondent (the ab) from committing any act of domestic violence, aiding or abetting such acts, or entering the victim's place of employment or other frequented places.\n\n3. Right to file for maintenance: Section 20 of the Act allows the aggrieved person to seek an order from the Magistrate for payment of a monthly allowance for the maintenance and education of any children under their custody or control.\n\n4. Right to compensation: Section 22 of the Act entitles the aggrieved person to compensation from the respondent for any loss or injury suffered due to the act of domestic violence.\n","output_type":"stream"}],"execution_count":4}]}